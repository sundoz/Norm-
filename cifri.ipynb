{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sundoz\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(42)\n",
    "nb_ep=200\n",
    "batch_size=128\n",
    "verbose=1\n",
    "nb_classes=10\n",
    "opt=SGD()\n",
    "n_hidden=128\n",
    "val_split=0.2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,y_train),(X_test,y_test)=mnist.load_data()\n",
    "X_train=X_train.reshape(60000,784)\n",
    "X_test=X_test.reshape(10000,784)\n",
    "X_train=X_train.astype('float32')\n",
    "X_test=X_test.astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train/=255\n",
    "X_test/=255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape[0],'train samples')\n",
    "print(X_test.shape[0],'test samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=np_utils.to_categorical(y_train, nb_classes)\n",
    "y_test=np_utils.to_categorical(y_train, nb_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model= Sequential()\n",
    "model.add(Dense(10,input_shape=(784,)))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/30\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 1.4199 - acc: 0.6502 - val_loss: 0.9048 - val_acc: 0.8237\n",
      "Epoch 2/30\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.7993 - acc: 0.8260 - val_loss: 0.6617 - val_acc: 0.8553\n",
      "Epoch 3/30\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.6474 - acc: 0.8467 - val_loss: 0.5651 - val_acc: 0.8675\n",
      "Epoch 4/30\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.5744 - acc: 0.8593 - val_loss: 0.5117 - val_acc: 0.8750\n",
      "Epoch 5/30\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.5300 - acc: 0.8671 - val_loss: 0.4771 - val_acc: 0.8820\n",
      "Epoch 6/30\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.4994 - acc: 0.8721 - val_loss: 0.4527 - val_acc: 0.8860\n",
      "Epoch 7/30\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.4768 - acc: 0.8759 - val_loss: 0.4343 - val_acc: 0.8883\n",
      "Epoch 8/30\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.4592 - acc: 0.8796 - val_loss: 0.4199 - val_acc: 0.8913\n",
      "Epoch 9/30\n",
      "48000/48000 [==============================] - 1s 17us/step - loss: 0.4451 - acc: 0.8826 - val_loss: 0.4080 - val_acc: 0.8943\n",
      "Epoch 10/30\n",
      "48000/48000 [==============================] - 1s 16us/step - loss: 0.4333 - acc: 0.8853 - val_loss: 0.3985 - val_acc: 0.8959\n",
      "Epoch 11/30\n",
      "48000/48000 [==============================] - 1s 17us/step - loss: 0.4234 - acc: 0.8868 - val_loss: 0.3902 - val_acc: 0.8991\n",
      "Epoch 12/30\n",
      "48000/48000 [==============================] - 1s 16us/step - loss: 0.4149 - acc: 0.8886 - val_loss: 0.3832 - val_acc: 0.9008\n",
      "Epoch 13/30\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.4074 - acc: 0.8897 - val_loss: 0.3768 - val_acc: 0.9018\n",
      "Epoch 14/30\n",
      "48000/48000 [==============================] - 1s 17us/step - loss: 0.4008 - acc: 0.8914 - val_loss: 0.3715 - val_acc: 0.9022\n",
      "Epoch 15/30\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.3950 - acc: 0.8927 - val_loss: 0.3668 - val_acc: 0.9022\n",
      "Epoch 16/30\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.3898 - acc: 0.8940 - val_loss: 0.3623 - val_acc: 0.9031\n",
      "Epoch 17/30\n",
      "48000/48000 [==============================] - 1s 16us/step - loss: 0.3850 - acc: 0.8947 - val_loss: 0.3583 - val_acc: 0.9042\n",
      "Epoch 18/30\n",
      "48000/48000 [==============================] - 1s 16us/step - loss: 0.3806 - acc: 0.8959 - val_loss: 0.3547 - val_acc: 0.9043\n",
      "Epoch 19/30\n",
      "48000/48000 [==============================] - 1s 17us/step - loss: 0.3766 - acc: 0.8962 - val_loss: 0.3515 - val_acc: 0.9054\n",
      "Epoch 20/30\n",
      "48000/48000 [==============================] - 1s 17us/step - loss: 0.3730 - acc: 0.8972 - val_loss: 0.3484 - val_acc: 0.9047\n",
      "Epoch 21/30\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3696 - acc: 0.8980 - val_loss: 0.3458 - val_acc: 0.9057\n",
      "Epoch 22/30\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3665 - acc: 0.8985 - val_loss: 0.3431 - val_acc: 0.9063\n",
      "Epoch 23/30\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3635 - acc: 0.8991 - val_loss: 0.3407 - val_acc: 0.9073\n",
      "Epoch 24/30\n",
      "48000/48000 [==============================] - 1s 17us/step - loss: 0.3608 - acc: 0.8995 - val_loss: 0.3385 - val_acc: 0.9081\n",
      "Epoch 25/30\n",
      "48000/48000 [==============================] - 1s 16us/step - loss: 0.3582 - acc: 0.9006 - val_loss: 0.3362 - val_acc: 0.9085\n",
      "Epoch 26/30\n",
      "48000/48000 [==============================] - 1s 16us/step - loss: 0.3558 - acc: 0.9010 - val_loss: 0.3343 - val_acc: 0.9081\n",
      "Epoch 27/30\n",
      "48000/48000 [==============================] - 1s 16us/step - loss: 0.3535 - acc: 0.9016 - val_loss: 0.3326 - val_acc: 0.9086\n",
      "Epoch 28/30\n",
      "48000/48000 [==============================] - 1s 16us/step - loss: 0.3514 - acc: 0.9021 - val_loss: 0.3307 - val_acc: 0.9096\n",
      "Epoch 29/30\n",
      "48000/48000 [==============================] - 1s 16us/step - loss: 0.3493 - acc: 0.9028 - val_loss: 0.3292 - val_acc: 0.9095\n",
      "Epoch 30/30\n",
      "48000/48000 [==============================] - 1s 16us/step - loss: 0.3474 - acc: 0.9031 - val_loss: 0.3275 - val_acc: 0.9107\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(),\n",
    "              metrics=['accuracy'])\n",
    "history=model.fit(X_train,y_train,\n",
    "         batch_size=128, epochs=30,\n",
    "         verbose=1,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected activation_4 to have 2 dimensions, but got array with shape (60000, 10, 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-427f7c4e90e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[0;32m   1100\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1102\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    126\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    129\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected activation_4 to have 2 dimensions, but got array with shape (60000, 10, 10)"
     ]
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
